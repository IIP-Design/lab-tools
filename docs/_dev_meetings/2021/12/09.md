---
title: December 09, 2021
tags: [Commons Dev Failure, Multi-Region Deployment]
date: 2021-12-09
excerpt: "Minutes from the December 09, 2021 GPA Lab developer's meeting. In which we discuss AWS failures."
---

## Commons Dev Failure

Yesterday (Dec. 8), the Elasticsearch instance for the Commons dev API failed unexpectedly. Typically, when a node crashes another will maintain data integrity until failed node recovers. However, to reduce costs, we are only running one Elasticsearch node in development. This means that when it crashed, there was no sibling node to keep the API up.

Generally even this failed single node would recover and come back on line, albeit missing some data. However, this time for some unknown reason the failed node was unable unrecoverable. Michael was able to spin up a new node and restore from a previous snapshot but the data lost is lost.

We can replace the dev Elasticsearch instance with a copy of production, but this leads to an issue whereby we have a data mismatch between Prisma an Elasticsearch, so content on the front end does not match the content in publisher.

Michael opened a ticket with AWS support to try to identify the cause of the failure in this instance.

To prevent this from happening, in the future, we could run a second Elasticsearch node in development. However, given the relative lack of importance of development data and the fact that this has only happened once or twice before that solution seems like overkill. Rather we should run daily snapshots of the Elasticsearch instance and recover from the lasted snapshot in the event of a failure.

_Update (Dec. 14):_ Michael has not yet received a response from AWS Support, however the dev Elasticsearch is back up and running. Michael will switch the dev API back to the restored Elasticsearch instance in order to eliminate the mismatch between the dev database and API.

## Multi-Region/Multi-Cloud Deployment

On Tuesday (Dec. 7) a [large scale outage](https://aws.amazon.com/message/12721/) in the AWS us-east-1 region caused service disruptions for our users. Both downloads of documents from S3 and saving/publishing data became flaky and were not consistently succeeding. Given our architecture (which resides entirely in us-east-1), we didn't have much choice but to wait the issue out.

While exceedingly rare, these AWS outages do happen so the team discussed possible contingencies to counter such incidents in the future. Given the nature of the issue, the only real mitigation is a multi-region or multi-cloud deployment. In this setup we would be prepared to shift the whole application to a different region or cloud at a moment's notice.

The application code is portable and Michael has written a Terraform script to bring up the environment, however the main constraint is maintain data integrity during the shift. We can easily rebuild the application data from backups, but this comes with a high likelihood of data loss from the time between the last backup and the shift. As a result, the bare minimum required to maintain data integrity is a complete replicate of all system data in the target environment. In other words, we would have to run a secondary database, Elasticsearch cluster, and S3 bucket(s) in the target environment. Once an incident occurs in the primary environment, we would rebuild the application in the secondary environment and promote the replicate data sources to primary status.

There are some serious downsides to this approach namely:

- Complexity - Adding a second region means more to manage, maintain, and possibly mess up. Given how rare such region-wide outages are, we have to weight the added complexity against the potential benefit.
- Expense - Given that RDS makes up the bulk of our application cost, running a full replica in another environment would dramatically increase our costs.

The team also discussed how we want to handle backups. Currently Michael runs a daily cron job to take backups (in addition to the 30 day snapshots provided by AWS), however we don't have a lifecycle policy for these backups. A potential policy could look something like:

- Backup daily
- Store backups for a year
- After a year go down to one backup per month (and move to [Glacier Storage](https://aws.amazon.com/s3/storage-classes/glacier/))
- Delete entirely after a fixed period of time (3 years?)
