---
title: December 02, 2021
tags: [Post-Release Check, Amgov Migration]
date: 2021-12-02
excerpt: "Minutes from the December 02, 2021 GPA Lab developer's meeting. Focused on avoiding misconfigurations in production deployments."
---

## Post-Release Checks

**Description:** There have been a couple instances of service distribution or deployment hiccups caused by process breakdowns within the dev team. The most notable instance was the v5.10.0 release on November 10, which enabled users to subscribe to email notifications regarding playbooks. Unfortunately, one of the environment variables was set incorrectly in production. As a result, we had two SQS queues listening to the same event and those picked up by the wrong queue were terminated rather than initiating the email send. This event prompted the team to discuss how we can add additional guard rails to our deployment process and avoid repeating such mishaps.

### Environmental Variables

For the past few months, the dev team has been compiling checklists prior to each deployment. These checklists outline what actions must be taken in what order and have been immensely helpful in reducing mistakes. However, there are still some blind spots even when using the checklists, particularly when it comes to environmental variables.

While the [checklist for the Nov. 10 deployment]({{ '/dev_meetings/2021/11/09.html#commons-prod-deployment-checklist' | relative_url }}) did list the environmental variable to be updated, it did not provide their values. This was an intentional choice because environmental variables, by their very nature, can be sensitive and should generally not be disclosed. In this case the variable (ie. an SQS queue endpoint) was not sensitive and could have been openly shared.

Another problem is the way in which environmental variables are applied. They are manually set using the `kubctl` CLI rather than pulled from some controlled store. While arguably more secure, this approach is also more error prone, difficult to double check, and adds a bottle neck when deploying. It has also led to issues when Michael is out and nobody else is available to update the variables.

In an ideal solution, environmental variables would:

- Be easy to update from a devoted dashboard
- Be available for all members of the dev team to review and/or update
- Store values in a safe (i.e. encrypted) fashion until used

Michael suggested that as a first step, we could add a static config file defining the environment variables to a private repo in our AWS account's CodeCommit instance. This file could be edited/reviewed by multiple members of the dev team prior to builds.

Marek agreed that this was a decent intermediate step, but that storing an unencrypted plain text file of secret values (even if in a private instance of CodeCommit) goes against security best practices. He suggested that we could look into something like [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/) or [HashiCorp vaults](https://www.vaultproject.io/). These services provide developers with a dashboard to update/confirm the environmental variable values but still store them in an encrypted fashion and only decrypt them as needed. Michael confirmed that there is a Terraform plugin to manage secrets in this fashion and suspects that Jenkins can likewise integrate with AWS Secrets Manager.

**_Update (12/07/21):_** Michael reported that he found a Jenkins plugin that works with AWS Secrets Manager and has made good progress with integrating it into the pipeline. He believes that we can skip the static config file step altogether and move directly to

With this sort of environment variable solution in place, at least one developer would go into the secrets manager to validate the config file/secrets store prior to build.

### More Robust Communication

A much less consequential process breakdown was the v5.11.2 deployment on December 1. No user disruption occurred as a result of this breakdown, rather the deployment of a change to production was needlessly delayed.

Edwin tagged the v5.11.2 release of the Commons client at 10:37 am, however due to lack of adequate communication, the build wasn't run until 5:24 pm. Edwin assumed the Michael would run the build once he saw the release notification in the **#design-devs** Slack channel. Michael assumed that someone would notify him when the release was ready for building (as is usually the case). Terri and Marek saw the release notifications but were distracted by other tasks and assumed that the build had been run. In the end all these assumptions delayed the release by seven hours.

In the grand scheme of things a seven hour delay on a minor feature rollout is pretty trivial. However, it does point to an part of the deployment process that lacked clear operating procedures. Marek typically tags a release, checks with Michael whether he is clear to run a build, and then runs the build himself. Terri tags the release and then tells Michael which build and release number he should run, thereby adding an additional check in the system. Edwin was unaware of these back channel communications and so did not follow suit.

The team agreed that best way forward to post an explicit go/no-go message in the in the **#lab-devops** Slack channel. By putting these messages out in the open the rest of the team will have better situational awareness and may be able to catch mistakes before they happen. The message should specify 1) what build should be run, 2) what environment/branch/version to use, and 3) who should run the build. For example a message may look like:

> I'm about to run a prod build of commons using version 5.11.2, @michael am I clear to go?

or

> @michael please run a prod build of the client using version 5.11.2

Receipt of this message should be acknowledged with a response. For example:

> @marek go ahead and run the production build

or

> @marek sounds good, I've initiated the production build

Additionally, Edwin suggested adding some visual distinction environments for each environment in the Slack build notifications. This would be helpful as it can be difficult to tell at a glance what job is being run. We already use different colors for different notification types (ex. started, success, failure, aborted), which is a useful convention that we would like to keep. One possibility to prefixing the job name with the environment in all-caps. So rather than:

![A Slack notification message that reads "Started: Job commons-dev-server-v1.19 build #196 by Michael. More info at: https://xxxx/job/commons-dev-server-v1.19/196/". The word "started" at the beginning is emphasized. ]({{ '/assets/2021/12/slack-notification-current.png' | relative_url }})

We might see something like:

![A Slack notification message that reads "Dev: Job commons-dev-server-v1.19 build #196 started by Michael. More info at: https://xxxx/job/commons-dev-server-v1.19/196/". The words "dev" at the beginning and "started" following the job name are emphasized. ]({{ '/assets/2021/12/slack-notification-proposed.png' | relative_url }})

By immediately drawing attention to the environment it should be clear whether the correct job is running or not.

### Post-Deployment Testing

The team also discussed to what extent we should be testing in the production environment after a release. Certain types of regression testing are certainly possible and advisable after a production deployment. QA can and should go through the application to confirm that everything appears correctly and there are no obvious bugs or errors.

Functional testing, on the other hand, is much more difficult to do in a production environment in a way that is not disruptive to real users. We do not want the front page to show test posts even for or brief period of time and we certainly do not want to send out test notifications to real users. This means that many common user behaviors are extremely difficult to test especially as we continue to layer on additional functionality.

### More Advanced Interventions

A more advanced approach would be to launch new features behind a flag that limits which users can see it. This way we could roll out new features to admin users only for initial test, then a set of pilot users for user testing, and finally would make them generally available once we are confident they work as expected.

The main concern with feature flags is the added complexity and technical overhead that it adds to the application. If we were to go this route we should probably utilize a third party provider like [LaunchDarkly](https://launchdarkly.com/), [CloudBees Feature Management](https://www.cloudbees.com/products/feature-management), or [Flagsmith](https://docs.flagsmith.com/).

Another area that we can invest in is building more robust error boundaries. At the moment we rely on user reports for discovery of all but the most major bugs. Ideally the application itself could detect and notify us of errors and anomalous behavior. At a minimum we can make better use of the APM tools we already pay for, particularly New Relic.

## Post-america.gov Commons Notifications

**Description:** The team briefly discussed how to prepare migration of all america.gov emails to the fan.gov domain. The greatest point of impact of this change is on the newly released playbook subscriptions which sends messages from AWS SES as `no-reply@america.gov`.

We will have to replace this sender email with a new one (preferably a state.gov account). While the change itself is not problematic, we will need to verify the address with SES. Michael outlined the two options for [verifying emails for SES](https://docs.aws.amazon.com/ses/latest/DeveloperGuide/verify-email-addresses.html). Specifically we can either:

1. register the entire sending domain, or
1. register the sending email

Given that we are looking to create an inbox using the state.gov domain, it is highly unlikely that we will be able to register the entire domain. Instead we can register the address with minimal to no input from IRM (or other provisioning team).

Marek raised a concern that we will not be able to apply [DKIM](https://postmarkapp.com/guides/dkim) signatures to emails when using account-based verification. Additionally, we need to confirm that sending emails from SES would not violate the state.gov/fan.gov [DMARC](https://postmarkapp.com/guides/dmarc) and [SPF](https://postmarkapp.com/guides/spf) policies.

**Next Steps:** The first thing to do is to create an inbox we could use for Commons in both the state.gov and the fan.gov domains. Michael believes that Farshad has already started a conversation with IRM about provisioning a state.gov email address. We should regroup with him to identify the inbox name and then try to create an analogous one in fan.gov.

## Commons Quick Hits

- Terri is doing some more testing on the document data loss restoration script. She plans to submit a pull request for Marek to review by the end of the day today.
- She has added a final step to the script that compares the original Elasticsearch index to the restored index and saves diff to S3. This allows for a quick and easy review of the changes and should help assess whether or not the restoration completed successfully.
