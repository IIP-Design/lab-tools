---
title: June 22, 2021
tags: [Dev Domain, MVC, Style Guide, Accessibility]
date: 2021-06-22
---

## Commons Quick Hits

We started off the meeting by discussing some Commons items:

1. Terri reported that she was unable to add playbooks to the search results. She ran into the same problems that Edwin and Marek experienced with the playbook preview and frontend pages, respectively. The call to `getServerSideProps` on the results page initially returned the page props as undefined. As a result, the current user data (specifically the user token) wasn't passed to the results page. Without a means to verify that the user had access to internal only content, the results page only shows public content.
1. If we want the benefits of server-side rendering on pages, it seems like we'll need to get rid of the custom `_app` component (or at least the invocation of `getInitialProps` therein). Otherwise, `getServerSideProps` will fail and we'll need to re-run the necessary queries in the client.
1. Terri asked where we should start with more meaningful refactoring of the Commons codebase. There is a lot of old, redundant, and at times sloppy code in the code base. While we all recognize the issues, we don't have a good inventory of problems recorded that we can convert into actionable tickets. We should take the "see something say something" approach, writing down examples of code that should be pruned/updated as we see them. Then we can then assign these refactoring tasks in a more coherent and planned way.

## Development Domain

**Description:** Yesterday we acquired the domain `gpalab.digital` via AWS's Route 53 registrar. We've set it up to automatically renew the registration annually.

For some reason, the domain verification email wasn't hitting the design devops email listed in the registrant contact info. Therefore, Michael switched the email to the Lab team email. The domain is now registered, verified, and Michael has added [DNSSEC](https://www.icann.org/resources/pages/dnssec-what-is-it-why-important-2019-03-05-en) authentication.

**Next Steps:** We just need to start using the domain! A natural first step would be to set up a Commons testing environment that Temi can rebuild/tear down as needed for testing.

## Resolver MVC

**Description:** Terri advocated for making the GraphQL [resolvers on the Commons server](https://github.com/IIP-Design/content-commons-server/tree/develop/src/resolvers) "thinner" by moving more of their logic into controllers.

Terri suggested that the resolver should serve the purpose of a public API, in that they demonstrate the high-level actions of the system, while abstracting away the detailed implementation. This higher level of abstraction makes it easier to quickly identify expected behavior for each resolver.

The inspiration behind this suggestion is the [Model-View-Controller (MVC)](https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller) design pattern, which recommends separating internal data management from the presentation of data to and interaction with the user.

Following this pattern, the view layer generates the UI that an end user interacts with. These interactions (such as click events) are defined and registered in the view layer but passed to the controller, which coordinates the work steps and implements the application logic. Implementation of changes to the internal data (such as interaction with the database) happen in the model level.

## Breaking Up Component Files

**Description:** In the vein of Terri's points on refactoring Commons and thinner resolvers, Marek raised the question of how we determine when to break up a component file. He said that while there is no hard and fast rule, he starts to feel uncomfortable with a file once it reaches ~500 lines and or there are multiple (more than 2-3) helper functions defined next to the component code.

When a file reaches a point that he feels is unwieldy, Marek tries to pull out the helper function and business logic into a `utils.js` file adjacent to the component file. Terri agreed with the general approach, but did not like the name `utils.js` since it implied shared/common utility functions. She prefers the name `controller.js`.

The team agreed to break up larger components into a main file responsible for the UI and a `controller` file to handle the complex logic. Generally, components should **NOT** need a `controller` file. Only large components with a lot of logic will need to be broken up. To the extent possible, components should be "dumb" in that they simply take input props and return a UI. Data manipulation should occur elsewhere and not be part of the component logic.

To avoid the confusion of multiple `controller` files in the the explorer, we decided to namespace all files in a component's directory by prefixing them with the component name. For example, the directory for `MyComponent` would look like so:

```bash
components
│
└── MyComponent
    │
    ├── MyComponent.js # Principal file containing the JSX UI/presentation of the component.
    ├── MyComponent.controller.js # The advanced component logic.
    ├── MyComponent.module.scss # Component-specific style sheet.
    ├── MyComponent.mocks.js # Mocked data for this component's test files.
    └── MyComponent.test.js # Tests the main component file & advanced logic in the controller file.
```

True utility functions (i.e. those that can be reused elsewhere outside of one particular component) should be extracted and moved into the `lib` directory.

In addition to breaking up files, the team agreed that we like to take a more [functional approach](https://eloquentjavascript.net/1st_edition/chapter6.html) to our code. Specifically, large functions should be broken up into smaller building blocks that "do one thing and do it well". These building blocks can then be composed together to create more complex systems. One significant benefit of this approach is that the resulting functions are much easier to test (and reason about).

## Consolidating Mock Files

**Description:** Marek raised the question about whether defining mocks at the component level was the right approach. He argued that this dependence on multiple sets of mocks could lead to inconsistent . tests become flaky/dependent on correctness of the mocks

There are a couple ways to potentially address the issue:

- Create a single set of mocks at the root of the project that can be imported where ever they are needed. These mocks would be composed of specific parts of an expected API response, which could then be combined together to simulate a full API response. The benefit of this approach is that updates to mocked data would only have to happen in one place but would be applied equally everywhere potentially reducing false negative tests. On the other hand, it separates mock data from the tests using it, making tests a bit less intuitive to write.
- Use an API mocking library like [MirageJS](https://miragejs.com/) to simulate an actual API responses. The problem with this is that it dramatically increases the amount of code that we would have to maintain, and would likely be prone to falling out of date with the real state of the API.
- It would be great if the mocks were [automatically generated](https://www.apollographql.com/docs/apollo-server/testing/mocking/) like they can be using Apollo server. Sadly, we don't know of any easy way to implement this.

**Decision:** The team was fairly receptive to the idea of a single set of consolidated mocks for the entire repository. However, we recognize the amount of work it will take to implement something of the sort and don't believe that it is a priority at this moment.

## Testing Commons with Disabled Users

**Description:** Anna is continuing to work on arranging for accessibility-related testing sessions with disabled users in the Bureau. These tests will be invaluable for ensuring that our ongoing efforts to make the site accessible pass muster and to identify additional areas for improvement.

Anna asked for any limitations that may make the testing difficult to accomplish. The only one we could think of is that it would be much more difficult to coordinate if the user lack an america.gov or state.gov email address.

Questions to answer in preparation for the test:

- Will the testing be general or directed testing? - In the general model the user would be given access to the site with minimal instruction and asked to narrate as they explore. This would help to identify natural behavior patterns and quite likely uncover unexpected issues. In the guided exercise, we would give the user a series of tasks to complete and see if they are able to do so and how intuitively they were able to navigate the site. This test would focus on areas where we have added accessibility features and could help to validate our methods/practices. Time permitting, we should conduct both test types, most likely starting with a guided exercise and then allowing the user to explore the site as they desire.
- Will they be assessing the frontend, publisher, or both? - Ideally we would have them test both sides of the application. However, our accessibility efforts have focused on the front end as that portion of the site received more usage. Perhaps we should focus there initially and only move to Publisher after additional work or if the user has a need to
- What sort of assistive technology will they be making use of? - It would be good to know ahead of time whether the user needs a screen reader, keyboard navigation, high contrast settings, or some combination of the above. This can help us tailor our guided test and determine what we should focus on in the test cases.
- Which environment do we grant them access to? - We can provide access to either the development or beta environments. There doesn't seem to be an advantage to one over the other. It would be nice if we could give them a fresh environment synced to prod (perhaps using the new dev domain).

Ideally all developers and designers should be present as observers for during the test. We'll need to investigate best practices for screen shares when using assistive technology.

**Next Steps:** Temi, with input from UX, will write some test cases for the user(s) to run through during our testing session.

**Resources:**

- [Involving Users in Evaluating Web Accessibility](https://www.w3.org/WAI/test-evaluate/involving-users/)
- [How to Incorporate Users with Disabilities in UX Testing](https://www.deque.com/blog/incorporate-users-disabilities-ux-testing/)
- [Accessibility in User-Centered Design: Planning Usability Testing](http://www.uiaccess.com/accessucd/ut_plan.html)
- [Tips For Conducting Usability Studies With Participants With Disabilities](https://www.smashingmagazine.com/2018/03/tips-conducting-usability-studies-participants-disabilities/)
- [Usability Testing by People with Disabilities: Some Guerrilla Tactics](https://uxpamagazine.org/guerilla_tactics/)

**Addendum:** In a related matter, Terri mentioned that Clara had discussed arranging some 508 compliance/accessibility training for the UX designers. This would encourage us as a team to introduce best practices in the design phase, rather than needing to correct them during development or even after deployment.
