---
title: April 22, 2022
tags: [Commons, Prisma 3, Environments]
date: 2022-04-22
excerpt: "Minutes from the April 22, 2022 GPA Lab developer's meeting. In which we eagerly await Prisma 3."
---

## Prisma 1 => 3 Migration

**Description:** Terri wanted to go over the process for migrating from Prisma 1 to Prisma 3 in our test environment. We will run this test on the "QA" environment running at "commons.gpalab.digital". This migration requires some significant changes to the data as well as a move from MySQL to Postgres. Terri has written a script that she runs locally to accomplish these tasks and wanted to make sure we had everything accounted for when attempting to do the same in our cloud-based environments.

**Migration Steps:**

1. Merge the `develop` branch of the server into the `debt` branch to ensure that everything is up to date and we don't have merge conflicts later one.
1. Push up the `debt` branch.
1. Stand up the Postgres database - already complete.
1. Migrate data from MySQL to Postgres and transform for use with Prisma 3.
1. Build the server off of the `debt` branch, pointing to the new Postgres database.

The final migration step takes the form of a script to be executed from the server. This script takes to following steps:

1. Copies pre-deployment schema (A best approximation of our current schema as adapted to Prisma 3).
1. Uses this schema to prepare the target Postgres database, creating the necessary tables and constraints
1. Copies the current MySQL data over into a "shadow DB" where the data can be staged and transformed before moving into the target database.
1. Runs a number of scripts against the shadow DB to make it compliant with the constraints in the target DB.
1. Copy the data from the shadow DB to the target DB.
1. Compare the schema to the DB (prompt to migrate again on mismatch)
1. Update the foreign keys used in the new schema.
1. Run Prisma migrate to generate a query to add the new foreign keys. At this point Prisma cannot tell that we are renaming existing foreign keys. As such it acts as if we are deleting the old ones and adding new ones, which has the effect of dropping all the previous data for the exiting foreign keys. For this reason, we save the query for further modification, but do not execute it.
1. Modified the generated query to rename (rather than drop) the old foreign keys and run it against the target DB.
1. Final check that the target DB matches the schema
1. Deletes the shadow DB

When we run the script in an actual environment, we would replace the portions that connect to database Docker containers with the appropriate database on RDS. Michael may create a Jenkins job to run this script so that we have all the steps documented and accounted for when we move to deploying in production.

## Clarification on Environment

**Description:** Marek pointed out that there seems to be some confusion over our where various resources reside across our two AWS sub-accounts and asked for some clarification. Both he and Terri believed that the "QA" environment is in the secondary `gpa-lab2` account. However, Michael stated that at the moment all Commons instances resides in the main `iip-design` account:

| Environment | Current AWS Account | Potential Future AWS Account |
| ----------- | ------------------- | ---------------------------- |
| Prod        | iip-design          | iip-design                   |
| Beta        | iip-design          | iip-design                   |
| QA          | iip-design          | gpa-lab2                     |
| Dev         | iip-design          | gpa-lab2                     |

The team discussed the desired end state and how to properly organize the environments. We believe that we will require a minimum of three environments:

1. The production environment in our main account.
1. A beta environment, as close to production as possible also in the main account. This would be used as a final step in regression/integration testing.
1. A development environment (whether we call is dev or qa or staging doesn't really matter) in the secondary account. This is where the bulk of testing would occur.

Whether or not we want a permanent fourth (QA) environment is an open question. The main benefit of this environment is that it would allow Michael to test infrastructural changes without impacting ongoing development. Michael is also working on a Terraform script to quickly spin up new instances of Commons. With this in place, we may be able to get to a state where we can spin up additional (ephemeral) testing environments in the `gpa-lab2` account as needed (whether to test a feature, a hotfix, or infrastructural changes).

The URLs used by each site is another open question. Marek and Terri advocated name-spacing each sub-account with a distinct URL. That way, if we see a `gpalab.digital` domain we know we are looking at something in `gpa-lab2` while `america.gov` indicates something in `iip-design`.

| Environment | Current URL              | Potential Future URL       |
| ----------- | ------------------------ | -------------------------- |
| Prod        | commons.america.gov      | commons.america.gov        |
| Beta        | commons-beta.america.gov | commons-beta.america.gov   |
| QA          | commons.gpalab.digital   | commons.gpalab.digital     |
| Dev         | commons-dev.america.gov  | commons-dev.gpalab.digital |

Currently, we use Route 53 in the main account to manage the `gpalab.digital` domain everywhere it is used (including the Talking Points dev instance which resides in IRM's AWS account). Should we decided to use this domain for development sites in the sub-account, we would migrate the Route 53 configurations into the sub-account. This would not only provided a better mental model about where everything resides but also facilitate the automation of DNS creation and management via Terraform.

Michael wondered whether having development environments on the `gpalab.digital` domain could cause issues in the case that we need to test with users outside of the team. Since the `gpalab.digital` is intended for our GPA Lab-internal uses only, sharing links to sites on this domain may raise questions as to why we are maintaining this domain. Marek and Terri did not feel that this is problematic as development sites still fall squarely under the umbrella of internal use and hence should be fair game.

## Duplicate Data

**Description:** Terri identified an issue with certain properties on a content type (for example the `content` node on a `documentFile`) in the current database. Namely, the same value for this property appears in the database numerous times. In each case only one instance is associated with a parent element. Terri believes that this is caused because of the one-to-many relationship between this property and it's parent. As such, each time the item is updated a new child node is created (rather than updating the existing one).

If this theory is correct, resolution is pretty straight forward. Rather than creating a new node on update, these properties should be upserted. Alternately, if the parent document is being deleted, then the delete should cascade down to it's children. By setting the relationship in the schema to a one-to-one relationship Prima 3 should alleviate this issue and prevent it's recurrence. As for the existing nodes that have a null value in the foreign key field, they can be safely deleted as they represent a previous state of the given node and as such are no longer needed. Terri will investigate the issue a bit further and include a cleanup of these orphaned nodes in the migration script (should it be appropriate).
