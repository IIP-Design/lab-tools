---
title: July 13, 2021
tags: [Jenkins, Commons]
date: 2021-07-13
---

## Commons Quick Hits

- The team received a Salesforce ticket (case #00187542) from a colleague in Legislative Affairs who was unable to find Press Guidance on Commons. We suspect that the the users was unaware that they need to sign in to see the press guidance.

## ECR Login Error on Jenkins

**Description:** Michael believes that he may have a solution for the ECR login error that we frequently experience while running Jenkins builds. The access token generated by the ECR token is stored in a Docker config file that is referenced at the time of the login attempt. Michael believes that the presence of this file from the previous build leads Jenkins to believe that the token has already been fetched and thereby prevents a token refresh.

A possible solution to this is to delete the config file upon the completion of every build. This will force Jenkins to always retrieve a fresh token on every build.

## Commons Prod Deployment

**Update Window:** Saturday July 17, 10:00 - 16:00

**Personnel:** Terri, Michael, Temi, Marek

**Order of Operation:**

- Add playbooks index to Elasticsearch (prior to build)
- Add type properties to language, owner, and taxonomy (term) indices (prior to build)
- Tag the release (minor version, can do the day before - Friday July, 16)
- Take snapshots of all data
- Push code
- Update Elasticsearch to v7
- Remap videos in Elasticsearch
- Run Prisma deploy
- Run GraphQL to seed updated data
  - Add policy priorities
  - Add playbook content type to Campaigns team
- Update the `ui.json` file IIP Static Assets repo to display the latest playbooks section

**Notes:**

- Also need to update the S3 permissions to allow copying of assets from authoring to production. This should probably be done ahead of time
- Contingency plan if something goes wrong is to roll back to the previous version.
- The team discussed whether we want to keep all three components (client, server, api) at the same version. We agreed that this is not necessary and we are fine with divergent version numbers.
- Changes will be tested on the live production site by publishing and then rapidly un-publishing a playbook. Ideally, we could test things like in production without it appearing for users (i.e. an A-B deployment that we when roll over to).

## Weekend Site Outage

**Description:** On Saturday (July 10) Commons (and Talking Points) when down for several hours. Users were greeted with a `400 Bad Request` error with the message `The plain HTTP request was sent to HTTPS port`.

Michael initially thought it was an issue with Kubernetes, but soon saw that all the services in the cluster were running fine. Further investigation showed that unencrypted traffic was being sent from Cloudflare to port 443 on the elastic load balancer. This resulted in failed requests for nearly all traffic to both Commons and the Talking Points input platform, as seen by this screenshot from the Cloudflare analytics dashboard:

![A chart displaying instances of 400 Bad Request errors on america.gov properties between 12:01 am on Friday July 9 and 6:00pm Sunday July 11. The chart starts flat near zero. Around 11:00 pm on Friday the errors dramatically increase to around 425 instances. The rate of errors stays between 400 and 700 instance per hour until 10:00 pm on Saturday when they drop back down to zero. The total number of errors reported during this time period is 12,860.]({{ '/assets/2021/07/cloudflare-traffic.png' | relative_url }})

**Incident Timeline:**

09:58 - Lab team is forwarded report from the Dubai Hub and IOs in NEA that Content Commons is down (case #00187251).

14:35 - Received notification that the Talking Points input platform is showing the same error (case #00187254).

19:48 - Terri sees the Salesforce tickets in her email.

21:39 - The site is back up.

Michael isn't sure what the root cause of the problem was, but suspects that it has to do with Cloudflare rotating the certificates for our domains. He addressed it by putting a certificate on the load balancer and terminating traffic there, but has reached out to Cloudflare support to track down the root cause.

A full incident report will be compiled when we have a full understanding of what happened and how to prevent it in the future.
