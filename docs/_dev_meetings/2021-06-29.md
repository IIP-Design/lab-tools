---
title: June 29, 2021
tags: [security incident]
date: 2021-06-29
---

## Security Incident Fallout

The team used this time to regroup and discuss the security vulnerability that was reported and patched on last Friday (June 25). Michael was still out of the office, which limited the discussion on some points, and we'll have to revisit some points when he returns.

We began by discussing the security incident report compiled by Marek. The team was unanimous in it's belief that we should not try to spin or marginalize the severity of to the reported vulnerability in the report. The report is intended to be a clear eyed assessment of the incident, our teams response, and areas for improvement. Watering down the report will only serve to diminish it's value as a learning tool and harm the team's credibility. Putting the team in a positively light for their rapid and thorough response is appropriate in communications with leadership, but should not be the focus of the report.

While the problem itself went undetected by the team, the underlying causes were recognized as problematic and listed as technical debt issues to resolve. Perhaps the team can use this fact as a illustration for the importance of addressing technical debt in our discussion with leadership. As a team, we need to be firmer in our justifications for working on technical debt and pushing back on unreasonable time estimates.

Terri suggested that it might be helpful to understand the development timelines under which other teams in the Bureau/Department operate. A fair comparison, however, would requires some knowledge of the sort of applications are they building on the requirements received.

The lack of time for addressing technical debt, however, was not the sole cause of this vulnerability. There were opportunities to catch this problem ourselves. For example;

- These changes went through code review and the most critical issue was not spotted
- We have no unit tests on this critical part of the application. Such tests may have alerted us to the issue much sooner.

## Remediation Steps

Marek compiled a list of actions the team should take to complete the remediation effort in response to this vulnerability. He asked the team to review his list and together they came up with the following task list:

- Add a check for the public user to the `requireLogin` function - Complete ‚úÖ
- Disable introspection in production - In progress üöß
- Wrap all operations in authentication functions - To do üìù
- Block all access to the GraphQL endpoint outside of the application - To do üìù
- Add a security policy to the site - To do üìù
- Add unit tests authentication flow on the server (and API) - To do üìù
- Eliminate the public user (after splitting public and internal content into two buckets) - To do üìù

These steps aim to:

1. Address the immediate bug
1. Harden the GraphQL endpoint making it more resistant to other attacks
1. Improve our ability to catch issues ahead of time and respond once detected
1. Simplify the codebase reducing the likelihood of unexpected edge cases

## Testing

Temi mentioned that in one of her previous jobs, they had tools to automatically test their API endpoints. These scripts ran regularly (approximately once a month) to ensure that the endpoints were working as expected. She did not know the specific tool used.

Marek concurred that such testing would be helpful, but cautioned that we can only test what we know we need to test. In other words, this critical bug (and such bugs in general) was the result of an unexpected interaction between two parts of the application. Each component in isolation worked as intended and was not a vulnerability in and out of itself. However, when combined, these two components were critically flawed.

The team posited that perhaps some integration testing (as opposed to an over reliance on unit testing) would help to identify these system interface bugs. By modelling/simulating higher level interactions with the application, we could potentially identify problematic edge cases sooner.

## Forensics

Marek mentioned that he was interested in learning how we can conduct forensics on an issue like this. Nobody present was sure of how such and analysis could/should be performed, but Michael may have a better understanding.

After patching the vulnerability, Marek did look at traffic to the endpoint in the Cloudflare dashboard. He saw nothing amiss and no changes to traffic but but wasn't sure if that is a reasonable way to analyze the threat. Presumably there are better ways to get detailed information about interactions with the endpoint such as a retrospective analysis of the logs.

If we wanted to get very fancy about it, the team could look into automated threat hunting using artificial intelligence to detect anomalous behavior. Of course this would require an investment of time and money outsized to the benefits provided. Given the relatively insensitive content on the site, the tradeoff between time spent on improving the application and time spent investigating potential incidents must be considered.

Here are some Cloud Academy courses that may be helpful in planning future responses or forensic efforts:

- [Cloud Governance, Risk, and Compliance](https://cloudacademy.com/course/cloud-governance-risk-and-compliance/course-introduction-14/)
- [Intrusion Detection and Prevention on Amazon Web Services](https://cloudacademy.com/course/intrusion-detection-and-prevention-on-amazon-web-services/introduction-87/)
- [Cloud Incident Response & Forensics: Introductory Lab](https://cloudacademy.com/lab/cloud-incident-response-forensics-introductory-lab/)
- [Cloud Incident Response & Forensics: Foundation Lab](https://cloudacademy.com/lab/cloud-incident-response-forensics-foundation-lab/)
- [Cloud Incident Response & Forensics: Intermediate Lab](https://cloudacademy.com/lab/cloud-incident-response-forensics-intermediate-lab/)

The three labs listed at the end are set up in the Azure environment, but the lessons may be relevant for all cloud environments.

## Publication of Report

Marek suggested that the team should publish the Security Incident Report in full once all of the remediation steps have been completed. He thinks that placing the postmortem (and future such reports) in the the Lab Tools repo alongside the dev meeting notes will make the resources more readily available and usable. This would:

1. Serve as an official record of incidents and the team's response therein
1. Facilitate the transfer of knowledge between team members (current and future)
1. Provide point reference for decisions made and lessons learned
1. Encourage accountability and best practices with regards to reporting and resolving issues

There is some validity to the counterargument that publishing this information publicly provides potential attackers with a greater understanding of our systems. That said, the code is already fully open source and the benefits of preserving this knowledge probably outweigh any potential concerns. With that said, care must be taken to ensure that all remediation steps are completed before any specific information about the particular vulnerability is released.

The publication of bugs and remediation efforts surrounding them is a key component of the thriving open source community. It is common practice for development teams to publish a security blog detailing known security issues. See examples from:

- [GitHub](https://github.blog/category/security/)
- [Google](https://security.googleblog.com)
- [Microsoft](https://https://msrc-blog.microsoft.com/)
- [Mitre's CVE List](https://cve.mitre.org/)
